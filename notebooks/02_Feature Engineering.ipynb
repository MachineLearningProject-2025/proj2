{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19bb2bd",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e12e5",
   "metadata": {},
   "source": [
    "## Î™®Îìà ÏûÑÌè¨Ìä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124421ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae808ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a0bff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5439a31",
   "metadata": {},
   "source": [
    "## Îç∞Ïù¥ÌÑ∞ ÏûÑÌè¨Ìä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2f5e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../dataset/train.csv')\n",
    "test_df = pd.read_csv('../dataset/test.csv')\n",
    "submission_df = pd.read_csv('../dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ca747",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features = [\n",
    "    'len_diff', 'punc_diff',\n",
    "    'sent_count_diff', \n",
    "    'lexical_div_diff',\n",
    "    'repetition_diff', \n",
    "    # 'subjectivity_diff',\n",
    "    'comma_ratio_diff', \n",
    "    # 'avg_word_len_diff'\n",
    "]\n",
    "\n",
    "target_words = [\n",
    "    'company', 'brace', 'knee', 'progression', \n",
    "    'apologize', 'sorry'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42d58eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    df['len_a'] = df['response_a'].str.len()\n",
    "    df['len_b'] = df['response_b'].str.len()\n",
    "    df['punc_a'] = df['response_a'].apply(lambda x: len(re.findall(r'[!?,;:]', str(x))))\n",
    "    df['punc_b'] = df['response_b'].apply(lambda x: len(re.findall(r'[!?,;:]', str(x))))\n",
    "    df['sent_a'] = df['response_a'].apply(lambda x: len(re.findall(r'[.!?]', str(x))))\n",
    "    df['sent_b'] = df['response_b'].apply(lambda x: len(re.findall(r'[.!?]', str(x))))\n",
    "\n",
    "    def ling_feat(text):\n",
    "        words = re.findall(r'\\b\\w+\\b', str(text).lower())\n",
    "        uniq = set(words)\n",
    "        lex_div = len(uniq)/(len(words)+1e-9)\n",
    "        repetition = 1 - len(uniq)/(len(words)+1e-9)\n",
    "        blob = TextBlob(str(text))\n",
    "        subj = blob.sentiment.subjectivity\n",
    "        return lex_div, repetition, subj\n",
    "    \n",
    "    for side in ['a', 'b']:\n",
    "        df[[f'lex_{side}', f'rep_{side}', f'subj_{side}']] = df[f'response_{side}'].apply(\n",
    "            lambda x: pd.Series(ling_feat(x))\n",
    "        )\n",
    "    \n",
    "    df['comma_a'] = df['response_a'].apply(lambda x: len(re.findall(r'[;,]', str(x))) / (len(x.split()) + 1e-9))\n",
    "    df['comma_b'] = df['response_b'].apply(lambda x: len(re.findall(r'[;,]', str(x))) / (len(x.split()) + 1e-9))\n",
    "    # df['avglen_a'] = df['response_a'].apply(lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split()) > 0 else 0)\n",
    "    # df['avglen_b'] = df['response_b'].apply(lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split()) > 0 else 0)\n",
    "\n",
    "    # diff features\n",
    "    df['len_diff'] = df['len_a'] - df['len_b']\n",
    "    df['punc_diff'] = df['punc_a'] - df['punc_b']\n",
    "    df['sent_count_diff'] = df['sent_a'] - df['sent_b']\n",
    "    df['lexical_div_diff'] = df['lex_a'] - df['lex_b']\n",
    "    df['repetition_diff'] = df['rep_a'] - df['rep_b']\n",
    "    # df['subjectivity_diff'] = df['subj_a'] - df['subj_b']\n",
    "    df['comma_ratio_diff'] = df['comma_a'] - df['comma_b']\n",
    "    # df['avg_word_len_diff'] = df['avglen_a'] - df['avglen_b']\n",
    "    \n",
    "    # --- keyword presence ---\n",
    "    text_cols = df[['prompt', 'response_a', 'response_b']].astype(str).agg(' '.join, axis=1)\n",
    "    for word in target_words:\n",
    "        df[f'contains_{word}'] = text_cols.str.contains(fr'\\b{word}\\b', case=False, na=False).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3256954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_features(train_df)\n",
    "test_df = get_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ec1b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'] = np.select(\n",
    "    [train_df['winner_model_a']==1, train_df['winner_model_b']==1, train_df['winner_tie']==1],\n",
    "    [0,1,2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fba3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_features = [f'contains_{w}' for w in target_words]\n",
    "all_features = significant_features + keyword_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b886abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>len_a</th>\n",
       "      <th>...</th>\n",
       "      <th>lexical_div_diff</th>\n",
       "      <th>repetition_diff</th>\n",
       "      <th>comma_ratio_diff</th>\n",
       "      <th>contains_company</th>\n",
       "      <th>contains_brace</th>\n",
       "      <th>contains_knee</th>\n",
       "      <th>contains_progression</th>\n",
       "      <th>contains_apologize</th>\n",
       "      <th>contains_sorry</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096081</td>\n",
       "      <td>0.096081</td>\n",
       "      <td>-0.022029</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124652</td>\n",
       "      <td>0.124652</td>\n",
       "      <td>-0.010712</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142640</td>\n",
       "      <td>-0.142640</td>\n",
       "      <td>-0.031418</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103679</td>\n",
       "      <td>0.103679</td>\n",
       "      <td>0.016538</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123966</td>\n",
       "      <td>0.123966</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57472</th>\n",
       "      <td>4294656694</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>claude-1</td>\n",
       "      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n",
       "      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n",
       "      <td>[\"Here is how that mnemonic represents the dig...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183474</td>\n",
       "      <td>-0.183474</td>\n",
       "      <td>0.073024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57473</th>\n",
       "      <td>4294692063</td>\n",
       "      <td>claude-2.0</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>[\"In python, implement a naive Bayes with gaus...</td>\n",
       "      <td>[\"Here is an implementation of a naive Bayes c...</td>\n",
       "      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041789</td>\n",
       "      <td>-0.041789</td>\n",
       "      <td>0.018849</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57474</th>\n",
       "      <td>4294710549</td>\n",
       "      <td>claude-1</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>[\"is it unethical to work on building weapons?...</td>\n",
       "      <td>[\"Working on weapons technology raises some et...</td>\n",
       "      <td>[\"It depends on the context. Weapons can be us...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069617</td>\n",
       "      <td>0.069617</td>\n",
       "      <td>-0.017390</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57475</th>\n",
       "      <td>4294899228</td>\n",
       "      <td>palm-2</td>\n",
       "      <td>tulu-2-dpo-70b</td>\n",
       "      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n",
       "      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n",
       "      <td>[\"As an AI language model, I do not promote or...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>-0.001637</td>\n",
       "      <td>0.010063</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57476</th>\n",
       "      <td>4294947231</td>\n",
       "      <td>gemini-pro-dev-api</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>[\"three kids eat three apples in three days, h...</td>\n",
       "      <td>[\"27 apples\"]</td>\n",
       "      <td>[\"If three kids eat three apples in three days...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.057143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57477 rows √ó 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             model_a              model_b  \\\n",
       "0           30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1           53567           koala-13b           gpt-4-0613   \n",
       "2           65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3           96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4          198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "...           ...                 ...                  ...   \n",
       "57472  4294656694          gpt-4-0613             claude-1   \n",
       "57473  4294692063          claude-2.0     llama-2-13b-chat   \n",
       "57474  4294710549            claude-1           alpaca-13b   \n",
       "57475  4294899228              palm-2       tulu-2-dpo-70b   \n",
       "57476  4294947231  gemini-pro-dev-api   gpt-4-1106-preview   \n",
       "\n",
       "                                                  prompt  \\\n",
       "0      [\"Is it morally right to try to have a certain...   \n",
       "1      [\"What is the difference between marriage lice...   \n",
       "2      [\"explain function calling. how would you call...   \n",
       "3      [\"How can I create a test set for a very rare ...   \n",
       "4      [\"What is the best way to travel from Tel-Aviv...   \n",
       "...                                                  ...   \n",
       "57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n",
       "57473  [\"In python, implement a naive Bayes with gaus...   \n",
       "57474  [\"is it unethical to work on building weapons?...   \n",
       "57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n",
       "57476  [\"three kids eat three apples in three days, h...   \n",
       "\n",
       "                                              response_a  \\\n",
       "0      [\"The question of whether it is morally right ...   \n",
       "1      [\"A marriage license is a legal document that ...   \n",
       "2      [\"Function calling is the process of invoking ...   \n",
       "3      [\"Creating a test set for a very rare category...   \n",
       "4      [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "...                                                  ...   \n",
       "57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n",
       "57473  [\"Here is an implementation of a naive Bayes c...   \n",
       "57474  [\"Working on weapons technology raises some et...   \n",
       "57475  [\"Bromadiolone is a rodenticide which is most ...   \n",
       "57476                                      [\"27 apples\"]   \n",
       "\n",
       "                                              response_b  winner_model_a  \\\n",
       "0      [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1      [\"A marriage license and a marriage certificat...               0   \n",
       "2      [\"Function calling is the process of invoking ...               0   \n",
       "3      [\"When building a classifier for a very rare c...               1   \n",
       "4      [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "...                                                  ...             ...   \n",
       "57472  [\"Here is how that mnemonic represents the dig...               1   \n",
       "57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n",
       "57474  [\"It depends on the context. Weapons can be us...               1   \n",
       "57475  [\"As an AI language model, I do not promote or...               0   \n",
       "57476  [\"If three kids eat three apples in three days...               1   \n",
       "\n",
       "       winner_model_b  winner_tie  len_a  ...  lexical_div_diff  \\\n",
       "0                   0           0   4538  ...         -0.096081   \n",
       "1                   1           0   3114  ...         -0.124652   \n",
       "2                   0           1    921  ...          0.142640   \n",
       "3                   0           0   3182  ...         -0.103679   \n",
       "4                   1           0   1300  ...         -0.123966   \n",
       "...               ...         ...    ...  ...               ...   \n",
       "57472               0           0    396  ...          0.183474   \n",
       "57473               0           0   1707  ...          0.041789   \n",
       "57474               0           0   8683  ...         -0.069617   \n",
       "57475               1           0    435  ...          0.001637   \n",
       "57476               0           0     13  ...          0.666667   \n",
       "\n",
       "       repetition_diff  comma_ratio_diff  contains_company  contains_brace  \\\n",
       "0             0.096081         -0.022029                 0               0   \n",
       "1             0.124652         -0.010712                 0               0   \n",
       "2            -0.142640         -0.031418                 0               0   \n",
       "3             0.103679          0.016538                 0               0   \n",
       "4             0.123966          0.002994                 0               0   \n",
       "...                ...               ...               ...             ...   \n",
       "57472        -0.183474          0.073024                 0               0   \n",
       "57473        -0.041789          0.018849                 0               0   \n",
       "57474         0.069617         -0.017390                 0               0   \n",
       "57475        -0.001637          0.010063                 0               0   \n",
       "57476        -0.666667         -0.057143                 0               0   \n",
       "\n",
       "       contains_knee  contains_progression  contains_apologize  \\\n",
       "0                  0                     0                   0   \n",
       "1                  0                     0                   0   \n",
       "2                  0                     0                   0   \n",
       "3                  0                     0                   0   \n",
       "4                  0                     0                   0   \n",
       "...              ...                   ...                 ...   \n",
       "57472              0                     0                   0   \n",
       "57473              0                     0                   0   \n",
       "57474              0                     0                   0   \n",
       "57475              0                     0                   0   \n",
       "57476              0                     0                   0   \n",
       "\n",
       "       contains_sorry  label  \n",
       "0                   0      0  \n",
       "1                   0      1  \n",
       "2                   0      2  \n",
       "3                   0      0  \n",
       "4                   0      1  \n",
       "...               ...    ...  \n",
       "57472               0      0  \n",
       "57473               0      0  \n",
       "57474               0      0  \n",
       "57475               0      1  \n",
       "57476               0      0  \n",
       "\n",
       "[57477 rows x 36 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2fb0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[all_features]\n",
    "y = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "083078d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_diff</th>\n",
       "      <th>punc_diff</th>\n",
       "      <th>sent_count_diff</th>\n",
       "      <th>lexical_div_diff</th>\n",
       "      <th>repetition_diff</th>\n",
       "      <th>comma_ratio_diff</th>\n",
       "      <th>contains_company</th>\n",
       "      <th>contains_brace</th>\n",
       "      <th>contains_knee</th>\n",
       "      <th>contains_progression</th>\n",
       "      <th>contains_apologize</th>\n",
       "      <th>contains_sorry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>57477.000000</td>\n",
       "      <td>57477.000000</td>\n",
       "      <td>57477.000000</td>\n",
       "      <td>57477.000000</td>\n",
       "      <td>57477.000000</td>\n",
       "      <td>57477.000000</td>\n",
       "      <td>57477.000000</td>\n",
       "      <td>57477.000000</td>\n",
       "      <td>57477.000000</td>\n",
       "      <td>57477.000000</td>\n",
       "      <td>57477.000000</td>\n",
       "      <td>57477.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-8.343250</td>\n",
       "      <td>-0.093376</td>\n",
       "      <td>-0.103467</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>-0.000207</td>\n",
       "      <td>-0.008809</td>\n",
       "      <td>0.047097</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.007760</td>\n",
       "      <td>0.044505</td>\n",
       "      <td>0.056057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1098.046783</td>\n",
       "      <td>19.128123</td>\n",
       "      <td>14.044901</td>\n",
       "      <td>0.175519</td>\n",
       "      <td>0.175519</td>\n",
       "      <td>1.317062</td>\n",
       "      <td>0.211848</td>\n",
       "      <td>0.053664</td>\n",
       "      <td>0.061470</td>\n",
       "      <td>0.087747</td>\n",
       "      <td>0.206215</td>\n",
       "      <td>0.230034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-52573.000000</td>\n",
       "      <td>-792.000000</td>\n",
       "      <td>-660.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-255.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-427.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-0.087695</td>\n",
       "      <td>-0.086965</td>\n",
       "      <td>-0.021958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>416.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.086965</td>\n",
       "      <td>0.087695</td>\n",
       "      <td>0.021247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>43542.000000</td>\n",
       "      <td>1350.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.047619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           len_diff     punc_diff  sent_count_diff  lexical_div_diff  \\\n",
       "count  57477.000000  57477.000000     57477.000000      57477.000000   \n",
       "mean      -8.343250     -0.093376        -0.103467          0.000207   \n",
       "std     1098.046783     19.128123        14.044901          0.175519   \n",
       "min   -52573.000000   -792.000000      -660.000000         -1.000000   \n",
       "25%     -427.000000     -6.000000        -5.000000         -0.087695   \n",
       "50%        0.000000      0.000000         0.000000          0.000000   \n",
       "75%      416.000000      6.000000         4.000000          0.086965   \n",
       "max    43542.000000   1350.000000       545.000000          1.000000   \n",
       "\n",
       "       repetition_diff  comma_ratio_diff  contains_company  contains_brace  \\\n",
       "count     57477.000000      57477.000000      57477.000000    57477.000000   \n",
       "mean         -0.000207         -0.008809          0.047097        0.002888   \n",
       "std           0.175519          1.317062          0.211848        0.053664   \n",
       "min          -1.000000       -255.000000          0.000000        0.000000   \n",
       "25%          -0.086965         -0.021958          0.000000        0.000000   \n",
       "50%           0.000000          0.000000          0.000000        0.000000   \n",
       "75%           0.087695          0.021247          0.000000        0.000000   \n",
       "max           1.000000         18.047619          1.000000        1.000000   \n",
       "\n",
       "       contains_knee  contains_progression  contains_apologize  contains_sorry  \n",
       "count   57477.000000          57477.000000        57477.000000    57477.000000  \n",
       "mean        0.003793              0.007760            0.044505        0.056057  \n",
       "std         0.061470              0.087747            0.206215        0.230034  \n",
       "min         0.000000              0.000000            0.000000        0.000000  \n",
       "25%         0.000000              0.000000            0.000000        0.000000  \n",
       "50%         0.000000              0.000000            0.000000        0.000000  \n",
       "75%         0.000000              0.000000            0.000000        0.000000  \n",
       "max         1.000000              1.000000            1.000000        1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6bec332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature summary (first rows):\n",
      "                        dtype  is_binary  n_unique           var        skew  \\\n",
      "feature                                                                        \n",
      "len_diff                int64      False      6041  1.205707e+06   -1.124580   \n",
      "punc_diff               int64      False       313  3.658851e+02    6.378323   \n",
      "sent_count_diff         int64      False       255  1.972592e+02   -2.359540   \n",
      "lexical_div_diff      float64      False     55950  3.080705e-02    0.025245   \n",
      "repetition_diff       float64      False     55950  3.080705e-02   -0.025245   \n",
      "comma_ratio_diff      float64      False     51816  1.734652e+00 -165.425305   \n",
      "contains_company        int64       True         2  4.487974e-02    4.275875   \n",
      "contains_brace          int64       True         2  2.879821e-03   18.527488   \n",
      "contains_knee           int64       True         2  3.778502e-03   16.145383   \n",
      "contains_progression    int64       True         2  7.699548e-03   11.219919   \n",
      "contains_apologize      int64       True         2  4.252482e-02    4.417817   \n",
      "contains_sorry          int64       True         2  5.291572e-02    3.859933   \n",
      "\n",
      "                      outlier_ratio  low_variance           recommended  \n",
      "feature                                                                  \n",
      "len_diff                   0.078344         False          RobustScaler  \n",
      "punc_diff                  0.094942         False          RobustScaler  \n",
      "sent_count_diff            0.104424         False          RobustScaler  \n",
      "lexical_div_diff           0.064565         False          RobustScaler  \n",
      "repetition_diff            0.064565         False          RobustScaler  \n",
      "comma_ratio_diff           0.073421         False          RobustScaler  \n",
      "contains_company           0.047097         False  passthrough (binary)  \n",
      "contains_brace             0.002888         False  passthrough (binary)  \n",
      "contains_knee              0.003793         False  passthrough (binary)  \n",
      "contains_progression       0.007760         False  passthrough (binary)  \n",
      "contains_apologize         0.044505         False  passthrough (binary)  \n",
      "contains_sorry             0.056057         False  passthrough (binary)  \n",
      "\n",
      "Groups:\n",
      "  binary_cols: ['contains_company', 'contains_brace', 'contains_knee', 'contains_progression', 'contains_apologize', 'contains_sorry']\n",
      "  robust_cols: ['comma_ratio_diff', 'len_diff', 'lexical_div_diff', 'punc_diff', 'repetition_diff', 'sent_count_diff']\n",
      "  std_cols: []\n",
      "\n",
      "Scaled features head:\n",
      "   comma_ratio_diff  len_diff  lexical_div_diff  punc_diff  repetition_diff  \\\n",
      "0         -0.509879  3.952550         -0.550103   3.333333         0.550103   \n",
      "1         -0.247947 -0.634638         -0.713681  -0.750000         0.713681   \n",
      "2         -0.727202 -1.084223          0.816672  -1.250000        -0.816672   \n",
      "3          0.382777  1.921708         -0.593601   1.666667         0.593601   \n",
      "4          0.069289  0.626335         -0.709755   0.833333         0.709755   \n",
      "\n",
      "   sent_count_diff  contains_company  contains_brace  contains_knee  \\\n",
      "0         3.000000               0.0             0.0            0.0   \n",
      "1        -1.777778               0.0             0.0            0.0   \n",
      "2        -0.666667               0.0             0.0            0.0   \n",
      "3         1.222222               0.0             0.0            0.0   \n",
      "4         0.222222               0.0             0.0            0.0   \n",
      "\n",
      "   contains_progression  contains_apologize  contains_sorry  \n",
      "0                   0.0                 0.0             0.0  \n",
      "1                   0.0                 0.0             0.0  \n",
      "2                   0.0                 0.0             0.0  \n",
      "3                   0.0                 0.0             0.0  \n",
      "4                   0.0                 0.0             0.0  \n",
      "\n",
      "Scaled features summary (variance after scaling):\n",
      "comma_ratio_diff        929.308658\n",
      "len_diff                  1.696628\n",
      "lexical_div_diff          1.009858\n",
      "punc_diff                 2.540869\n",
      "repetition_diff           1.009858\n",
      "sent_count_diff           2.435299\n",
      "contains_company          0.044880\n",
      "contains_brace            0.002880\n",
      "contains_knee             0.003779\n",
      "contains_progression      0.007700\n",
      "contains_apologize        0.042525\n",
      "contains_sorry            0.052916\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Î∂ÑÏÑù Î∞è ÌòºÌï© Ïä§ÏºÄÏùºÎßÅ(Ïó¥Î≥ÑÎ°ú StandardScaler / RobustScaler / Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ)\n",
    "\n",
    "# X, all_features Î≥ÄÏàòÍ∞Ä Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïúÎã§Í≥† Í∞ÄÏ†ï\n",
    "df = X.copy()\n",
    "\n",
    "# Í∏∞Î≥∏ ÌÜµÍ≥ÑÎüâ & Ïù¥ÏÉÅÏπò/Î∂ÑÏÇ∞ Í≥ÑÏÇ∞\n",
    "summary = []\n",
    "for col in df.columns:\n",
    "  vals = df[col].dropna().astype(float)\n",
    "  n_unique = df[col].nunique(dropna=True)\n",
    "  is_binary = set(vals.unique()).issubset({0.0, 1.0}) or n_unique == 2\n",
    "  var = vals.var()\n",
    "  skew = vals.skew()\n",
    "  q1, q3 = vals.quantile(0.25), vals.quantile(0.75)\n",
    "  iqr = q3 - q1\n",
    "  lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "  outlier_mask = (vals < lower) | (vals > upper)\n",
    "  outlier_ratio = outlier_mask.sum() / max(len(vals), 1)\n",
    "  low_variance = var < 1e-4  # ÏûÑÍ≥ÑÍ∞íÏùÄ Îç∞Ïù¥ÌÑ∞Ïóê Îî∞Îùº Ï°∞Ï†ï Í∞ÄÎä•\n",
    "  high_skew = abs(skew) > 1.0\n",
    "\n",
    "  # Ïä§ÏºÄÏùºÎü¨ Í∂åÏû•: Ïù¥ÏßÑ Î≥ÄÏàòÎäî Í∑∏ÎåÄÎ°ú, Ïù¥ÏÉÅÏπòÍ∞Ä ÎßéÍ±∞ÎÇò ÏôúÎèÑÍ∞Ä ÌÅ∞ Í≤ΩÏö∞ Robust, ÏïÑÎãàÎ©¥ Standard\n",
    "  if is_binary:\n",
    "    recommended = 'passthrough (binary)'\n",
    "  elif outlier_ratio > 0.05 or high_skew:\n",
    "    recommended = 'RobustScaler'\n",
    "  else:\n",
    "    recommended = 'StandardScaler'\n",
    "\n",
    "  summary.append({\n",
    "    'feature': col,\n",
    "    'dtype': str(df[col].dtype),\n",
    "    'n_unique': n_unique,\n",
    "    'is_binary': is_binary,\n",
    "    'var': var,\n",
    "    'skew': skew,\n",
    "    'q1': q1,\n",
    "    'q3': q3,\n",
    "    'iqr': iqr,\n",
    "    'outlier_ratio': outlier_ratio,\n",
    "    'low_variance': low_variance,\n",
    "    'recommended': recommended\n",
    "  })\n",
    "\n",
    "summary_df = pd.DataFrame(summary).set_index('feature')\n",
    "print(\"Feature summary (first rows):\")\n",
    "print(summary_df[['dtype', 'is_binary', 'n_unique', 'var', 'skew', 'outlier_ratio', 'low_variance', 'recommended']].head(20))\n",
    "\n",
    "# Í∑∏Î£π Î∂ÑÎ•ò\n",
    "binary_cols = summary_df[summary_df['is_binary']].index.tolist()\n",
    "robust_cols = summary_df[summary_df['recommended'] == 'RobustScaler'].index.difference(binary_cols).tolist()\n",
    "std_cols = summary_df[summary_df['recommended'] == 'StandardScaler'].index.difference(binary_cols).tolist()\n",
    "\n",
    "print(\"\\nGroups:\")\n",
    "print(\"  binary_cols:\", binary_cols)\n",
    "print(\"  robust_cols:\", robust_cols)\n",
    "print(\"  std_cols:\", std_cols)\n",
    "\n",
    "# ColumnTransformer Íµ¨ÏÑ± (Îπà Í∑∏Î£πÏùÄ ÏÉùÎûµ)\n",
    "transformers = []\n",
    "if std_cols:\n",
    "  transformers.append(('std', StandardScaler(), std_cols))\n",
    "if robust_cols:\n",
    "  transformers.append(('robust', RobustScaler(), robust_cols))\n",
    "# passthrough for binary (Ïú†ÏßÄ)\n",
    "if binary_cols:\n",
    "  transformers.append(('passthrough_binary', 'passthrough', binary_cols))\n",
    "\n",
    "if not transformers:\n",
    "  raise RuntimeError(\"No features to transform. Check X/all_features.\")\n",
    "\n",
    "col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0)\n",
    "\n",
    "# fit + transform -> DataFrameÏúºÎ°ú Î≥µÏõê\n",
    "X_scaled_arr = col_transformer.fit_transform(df)\n",
    "# ColumnTransformer ÏàúÏÑúÎ•º Ïù¥Ïö©Ìï¥ Ïª¨ÎüºÎ™Ö Ïû¨Íµ¨ÏÑ±\n",
    "out_cols = []\n",
    "for name, _, cols in transformers:\n",
    "  if cols == 'passthrough' or name.startswith('passthrough'):\n",
    "    # passthrough ÏÇ¨Ïö©Ìïú Í≤ΩÏö∞, Ïã§Ï†ú Ïª¨Îüº ÏàúÏÑúÍ∞Ä ÏûÖÎ†• dfÏóêÏÑú Ïú†ÏßÄÎêòÎØÄÎ°ú colsÎäî Î¶¨Ïä§Ìä∏Î°ú Î∞õÏùå\n",
    "    out_cols.extend(cols if isinstance(cols, (list, tuple)) else list(cols))\n",
    "  else:\n",
    "    out_cols.extend(cols)\n",
    "X_scaled_df = pd.DataFrame(X_scaled_arr, columns=out_cols, index=df.index)\n",
    "\n",
    "# Í≤∞Í≥º ÏöîÏïΩ Ï∂úÎ†•\n",
    "print(\"\\nScaled features head:\")\n",
    "print(X_scaled_df.head())\n",
    "\n",
    "print(\"\\nScaled features summary (variance after scaling):\")\n",
    "print(X_scaled_df.var())\n",
    "\n",
    "# ÌïÑÏöî Ïãú ÌååÏùº/Î≥ÄÏàòÎ°ú Ï†ÄÏû•Ìï¥ Îã§Ïùå ÏÖÄÏóêÏÑú Ïû¨ÏÇ¨Ïö© Í∞ÄÎä•\n",
    "# Ïòà: X_scaled_df.to_csv('X_scaled_preview.csv', index=False)\n",
    "# ÎòêÎäî Ï†ÑÏó≠ Î≥ÄÏàòÎ°ú Î≥¥Ï°¥\n",
    "X_scaled_preview = X_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2dd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratifyÎ°ú ÌÅ¥ÎûòÏä§ ÎπÑÏú® Ïú†ÏßÄ\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled_df, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# --- 3. Î≤†Ïù¥Ïä§ Î™®Îç∏ Íµ¨ÏÑ± ---\n",
    "lr = LogisticRegression(max_iter=10000, multi_class='multinomial', solver='lbfgs', random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "lgb = LGBMClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- 4. Voting Ensemble ---\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('rf', rf), ('gb', gb), ('lgb', lgb)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# --- 5. ÌïôÏäµ ---\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# --- 6. Í≤ÄÏ¶ù ---\n",
    "y_pred = ensemble.predict(X_val)\n",
    "y_prob = ensemble.predict_proba(X_val)\n",
    "\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "loss = log_loss(y_val, y_prob)\n",
    "print(f\"\\n‚úÖ Validation Accuracy: {acc:.4f}\")\n",
    "print(f\"‚úÖ Validation LogLoss: {loss:.4f}\")\n",
    "\n",
    "# Ï∂îÍ∞Ä Î¶¨Ìè¨Ìä∏\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=['A win', 'B win', 'Tie']))\n",
    "\n",
    "# --- 7. ÌòºÎèô ÌñâÎ†¨ ÏãúÍ∞ÅÌôî ---\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Validation)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# --- 8. ÏµúÏ¢Ö ÏòàÏ∏° ---\n",
    "# test_df_scaled Ï§ÄÎπÑ\n",
    "X_test_scaled = X_scaled_df.loc[test_df.index] if len(test_df) == len(X_scaled_df) else X_scaled_df.iloc[:len(test_df)]\n",
    "test_pred = ensemble.predict_proba(X_test_scaled)\n",
    "\n",
    "# --- 9. Ï†úÏ∂ú ÌååÏùº ÏÉùÏÑ± ---\n",
    "submission_df['winner_model_a'] = test_pred[:, 0]\n",
    "submission_df['winner_model_b'] = test_pred[:, 1]\n",
    "submission_df['winner_tie'] = test_pred[:, 2]\n",
    "submission_df.to_csv('submission_final_ensemble.csv', index=False)\n",
    "\n",
    "print(\"\\nüéØ submission_final_ensemble.csv saved successfully!\")\n",
    "\n",
    "# --- 10. Feature Importance (LGBM Í∏∞Î∞ò) ---\n",
    "importances = pd.Series(lgb.feature_importances_, index=X_scaled_df.columns)\n",
    "plt.figure(figsize=(10,6))\n",
    "importances.nlargest(15).plot(kind='barh')\n",
    "plt.title(\"Top 15 Feature Importances (LightGBM)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
