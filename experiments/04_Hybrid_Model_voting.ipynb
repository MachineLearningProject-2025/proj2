{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6800762",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91830cb",
   "metadata": {},
   "source": [
    "## Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91ab6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b120cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afa3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffbd4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebed80a",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fccccdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../dataset/train.csv')\n",
    "test_df = pd.read_csv('../dataset/test.csv')\n",
    "submission_df = pd.read_csv('../dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864ce00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features = [\n",
    "    'len_diff', 'punc_diff',\n",
    "    'sent_count_diff', \n",
    "    'lexical_div_diff',\n",
    "    'repetition_diff', \n",
    "    # 'subjectivity_diff',\n",
    "    'comma_ratio_diff', \n",
    "    # 'avg_word_len_diff'\n",
    "]\n",
    "\n",
    "target_words = [\n",
    "    'company', 'brace', 'knee', 'progression', \n",
    "    'apologize', 'sorry'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06dd385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    df['len_a'] = df['response_a'].str.len()\n",
    "    df['len_b'] = df['response_b'].str.len()\n",
    "    df['punc_a'] = df['response_a'].apply(lambda x: len(re.findall(r'[!?,;:]', str(x))))\n",
    "    df['punc_b'] = df['response_b'].apply(lambda x: len(re.findall(r'[!?,;:]', str(x))))\n",
    "    df['sent_a'] = df['response_a'].apply(lambda x: len(re.findall(r'[.!?]', str(x))))\n",
    "    df['sent_b'] = df['response_b'].apply(lambda x: len(re.findall(r'[.!?]', str(x))))\n",
    "\n",
    "    def ling_feat(text):\n",
    "        words = re.findall(r'\\b\\w+\\b', str(text).lower())\n",
    "        uniq = set(words)\n",
    "        lex_div = len(uniq)/(len(words)+1e-9)\n",
    "        repetition = 1 - len(uniq)/(len(words)+1e-9)\n",
    "        blob = TextBlob(str(text))\n",
    "        subj = blob.sentiment.subjectivity\n",
    "        return lex_div, repetition, subj\n",
    "    \n",
    "    for side in ['a', 'b']:\n",
    "        df[[f'lex_{side}', f'rep_{side}', f'subj_{side}']] = df[f'response_{side}'].apply(\n",
    "            lambda x: pd.Series(ling_feat(x))\n",
    "        )\n",
    "    \n",
    "    df['comma_a'] = df['response_a'].apply(lambda x: len(re.findall(r'[;,]', str(x))) / (len(x.split()) + 1e-9))\n",
    "    df['comma_b'] = df['response_b'].apply(lambda x: len(re.findall(r'[;,]', str(x))) / (len(x.split()) + 1e-9))\n",
    "    # df['avglen_a'] = df['response_a'].apply(lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split()) > 0 else 0)\n",
    "    # df['avglen_b'] = df['response_b'].apply(lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split()) > 0 else 0)\n",
    "\n",
    "    # diff features\n",
    "    df['len_diff'] = df['len_a'] - df['len_b']\n",
    "    df['punc_diff'] = df['punc_a'] - df['punc_b']\n",
    "    df['sent_count_diff'] = df['sent_a'] - df['sent_b']\n",
    "    df['lexical_div_diff'] = df['lex_a'] - df['lex_b']\n",
    "    df['repetition_diff'] = df['rep_a'] - df['rep_b']\n",
    "    # df['subjectivity_diff'] = df['subj_a'] - df['subj_b']\n",
    "    df['comma_ratio_diff'] = df['comma_a'] - df['comma_b']\n",
    "    # df['avg_word_len_diff'] = df['avglen_a'] - df['avglen_b']\n",
    "    \n",
    "    # --- keyword presence ---\n",
    "    text_cols = df[['prompt', 'response_a', 'response_b']].astype(str).agg(' '.join, axis=1)\n",
    "    for word in target_words:\n",
    "        df[f'contains_{word}'] = text_cols.str.contains(fr'\\b{word}\\b', case=False, na=False).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7b6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_features(train_df)\n",
    "test_df = get_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb8d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'] = np.select(\n",
    "    [train_df['winner_model_a']==1, train_df['winner_model_b']==1, train_df['winner_tie']==1],\n",
    "    [0,1,2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d88ac9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_features = [f'contains_{w}' for w in target_words]\n",
    "all_features = significant_features + keyword_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa78c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[all_features]\n",
    "y = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d49d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature summary (first rows):\n",
      "                        dtype  is_binary  n_unique           var        skew  \\\n",
      "feature                                                                        \n",
      "len_diff                int64      False      6041  1.205707e+06   -1.124580   \n",
      "punc_diff               int64      False       313  3.658851e+02    6.378323   \n",
      "sent_count_diff         int64      False       255  1.972592e+02   -2.359540   \n",
      "lexical_div_diff      float64      False     55950  3.080705e-02    0.025245   \n",
      "repetition_diff       float64      False     55950  3.080705e-02   -0.025245   \n",
      "comma_ratio_diff      float64      False     51816  1.734652e+00 -165.425305   \n",
      "contains_company        int64       True         2  4.487974e-02    4.275875   \n",
      "contains_brace          int64       True         2  2.879821e-03   18.527488   \n",
      "contains_knee           int64       True         2  3.778502e-03   16.145383   \n",
      "contains_progression    int64       True         2  7.699548e-03   11.219919   \n",
      "contains_apologize      int64       True         2  4.252482e-02    4.417817   \n",
      "contains_sorry          int64       True         2  5.291572e-02    3.859933   \n",
      "\n",
      "                      outlier_ratio  low_variance           recommended  \n",
      "feature                                                                  \n",
      "len_diff                   0.078344         False          RobustScaler  \n",
      "punc_diff                  0.094942         False          RobustScaler  \n",
      "sent_count_diff            0.104424         False          RobustScaler  \n",
      "lexical_div_diff           0.064565         False          RobustScaler  \n",
      "repetition_diff            0.064565         False          RobustScaler  \n",
      "comma_ratio_diff           0.073421         False          RobustScaler  \n",
      "contains_company           0.047097         False  passthrough (binary)  \n",
      "contains_brace             0.002888         False  passthrough (binary)  \n",
      "contains_knee              0.003793         False  passthrough (binary)  \n",
      "contains_progression       0.007760         False  passthrough (binary)  \n",
      "contains_apologize         0.044505         False  passthrough (binary)  \n",
      "contains_sorry             0.056057         False  passthrough (binary)  \n",
      "\n",
      "Groups:\n",
      "  binary_cols: ['contains_company', 'contains_brace', 'contains_knee', 'contains_progression', 'contains_apologize', 'contains_sorry']\n",
      "  robust_cols: ['comma_ratio_diff', 'len_diff', 'lexical_div_diff', 'punc_diff', 'repetition_diff', 'sent_count_diff']\n",
      "  std_cols: []\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Î∂ÑÏÑù Î∞è ÌòºÌï© Ïä§ÏºÄÏùºÎßÅ(Ïó¥Î≥ÑÎ°ú StandardScaler / RobustScaler / Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ)\n",
    "\n",
    "# X, all_features Î≥ÄÏàòÍ∞Ä Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïúÎã§Í≥† Í∞ÄÏ†ï\n",
    "df = X.copy()\n",
    "\n",
    "# Í∏∞Î≥∏ ÌÜµÍ≥ÑÎüâ & Ïù¥ÏÉÅÏπò/Î∂ÑÏÇ∞ Í≥ÑÏÇ∞\n",
    "summary = []\n",
    "for col in df.columns:\n",
    "  vals = df[col].dropna().astype(float)\n",
    "  n_unique = df[col].nunique(dropna=True)\n",
    "  is_binary = set(vals.unique()).issubset({0.0, 1.0}) or n_unique == 2\n",
    "  var = vals.var()\n",
    "  skew = vals.skew()\n",
    "  q1, q3 = vals.quantile(0.25), vals.quantile(0.75)\n",
    "  iqr = q3 - q1\n",
    "  lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "  outlier_mask = (vals < lower) | (vals > upper)\n",
    "  outlier_ratio = outlier_mask.sum() / max(len(vals), 1)\n",
    "  low_variance = var < 1e-4  # ÏûÑÍ≥ÑÍ∞íÏùÄ Îç∞Ïù¥ÌÑ∞Ïóê Îî∞Îùº Ï°∞Ï†ï Í∞ÄÎä•\n",
    "  high_skew = abs(skew) > 1.0\n",
    "\n",
    "  # Ïä§ÏºÄÏùºÎü¨ Í∂åÏû•: Ïù¥ÏßÑ Î≥ÄÏàòÎäî Í∑∏ÎåÄÎ°ú, Ïù¥ÏÉÅÏπòÍ∞Ä ÎßéÍ±∞ÎÇò ÏôúÎèÑÍ∞Ä ÌÅ∞ Í≤ΩÏö∞ Robust, ÏïÑÎãàÎ©¥ Standard\n",
    "  if is_binary:\n",
    "    recommended = 'passthrough (binary)'\n",
    "  elif outlier_ratio > 0.05 or high_skew:\n",
    "    recommended = 'RobustScaler'\n",
    "  else:\n",
    "    recommended = 'StandardScaler'\n",
    "\n",
    "  summary.append({\n",
    "    'feature': col,\n",
    "    'dtype': str(df[col].dtype),\n",
    "    'n_unique': n_unique,\n",
    "    'is_binary': is_binary,\n",
    "    'var': var,\n",
    "    'skew': skew,\n",
    "    'q1': q1,\n",
    "    'q3': q3,\n",
    "    'iqr': iqr,\n",
    "    'outlier_ratio': outlier_ratio,\n",
    "    'low_variance': low_variance,\n",
    "    'recommended': recommended\n",
    "  })\n",
    "\n",
    "summary_df = pd.DataFrame(summary).set_index('feature')\n",
    "print(\"Feature summary (first rows):\")\n",
    "print(summary_df[['dtype', 'is_binary', 'n_unique', 'var', 'skew', 'outlier_ratio', 'low_variance', 'recommended']].head(20))\n",
    "\n",
    "# Í∑∏Î£π Î∂ÑÎ•ò\n",
    "binary_cols = summary_df[summary_df['is_binary']].index.tolist()\n",
    "robust_cols = summary_df[summary_df['recommended'] == 'RobustScaler'].index.difference(binary_cols).tolist()\n",
    "std_cols = summary_df[summary_df['recommended'] == 'StandardScaler'].index.difference(binary_cols).tolist()\n",
    "\n",
    "print(\"\\nGroups:\")\n",
    "print(\"  binary_cols:\", binary_cols)\n",
    "print(\"  robust_cols:\", robust_cols)\n",
    "print(\"  std_cols:\", std_cols)\n",
    "\n",
    "# ColumnTransformer Íµ¨ÏÑ± (Îπà Í∑∏Î£πÏùÄ ÏÉùÎûµ)\n",
    "transformers = []\n",
    "if std_cols:\n",
    "  transformers.append(('std', StandardScaler(), std_cols))\n",
    "if robust_cols:\n",
    "  transformers.append(('robust', RobustScaler(), robust_cols))\n",
    "# passthrough for binary (Ïú†ÏßÄ)\n",
    "if binary_cols:\n",
    "  transformers.append(('passthrough_binary', 'passthrough', binary_cols))\n",
    "\n",
    "if not transformers:\n",
    "  raise RuntimeError(\"No features to transform. Check X/all_features.\")\n",
    "\n",
    "col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0)\n",
    "\n",
    "# fit + transform -> DataFrameÏúºÎ°ú Î≥µÏõê\n",
    "X_scaled_arr = col_transformer.fit_transform(df)\n",
    "# ColumnTransformer ÏàúÏÑúÎ•º Ïù¥Ïö©Ìï¥ Ïª¨ÎüºÎ™Ö Ïû¨Íµ¨ÏÑ±\n",
    "out_cols = []\n",
    "for name, _, cols in transformers:\n",
    "  if cols == 'passthrough' or name.startswith('passthrough'):\n",
    "    # passthrough ÏÇ¨Ïö©Ìïú Í≤ΩÏö∞, Ïã§Ï†ú Ïª¨Îüº ÏàúÏÑúÍ∞Ä ÏûÖÎ†• dfÏóêÏÑú Ïú†ÏßÄÎêòÎØÄÎ°ú colsÎäî Î¶¨Ïä§Ìä∏Î°ú Î∞õÏùå\n",
    "    out_cols.extend(cols if isinstance(cols, (list, tuple)) else list(cols))\n",
    "  else:\n",
    "    out_cols.extend(cols)\n",
    "X_scaled_df = pd.DataFrame(X_scaled_arr, columns=out_cols, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83f64804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comma_ratio_diff</th>\n",
       "      <th>len_diff</th>\n",
       "      <th>lexical_div_diff</th>\n",
       "      <th>punc_diff</th>\n",
       "      <th>repetition_diff</th>\n",
       "      <th>sent_count_diff</th>\n",
       "      <th>contains_company</th>\n",
       "      <th>contains_brace</th>\n",
       "      <th>contains_knee</th>\n",
       "      <th>contains_progression</th>\n",
       "      <th>contains_apologize</th>\n",
       "      <th>contains_sorry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.509879</td>\n",
       "      <td>3.952550</td>\n",
       "      <td>-0.550103</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.550103</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.247947</td>\n",
       "      <td>-0.634638</td>\n",
       "      <td>-0.713681</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.713681</td>\n",
       "      <td>-1.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.727202</td>\n",
       "      <td>-1.084223</td>\n",
       "      <td>0.816672</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>-0.816672</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.382777</td>\n",
       "      <td>1.921708</td>\n",
       "      <td>-0.593601</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.593601</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069289</td>\n",
       "      <td>0.626335</td>\n",
       "      <td>-0.709755</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.709755</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57472</th>\n",
       "      <td>1.690206</td>\n",
       "      <td>-0.190985</td>\n",
       "      <td>1.050461</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-1.050461</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57473</th>\n",
       "      <td>0.436278</td>\n",
       "      <td>-0.007117</td>\n",
       "      <td>0.239260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.239260</td>\n",
       "      <td>-1.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57474</th>\n",
       "      <td>-0.402496</td>\n",
       "      <td>8.451957</td>\n",
       "      <td>-0.398586</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>0.398586</td>\n",
       "      <td>10.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57475</th>\n",
       "      <td>0.232915</td>\n",
       "      <td>-0.633452</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.009375</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57476</th>\n",
       "      <td>-1.322622</td>\n",
       "      <td>-0.608541</td>\n",
       "      <td>3.816926</td>\n",
       "      <td>-0.583333</td>\n",
       "      <td>-3.816926</td>\n",
       "      <td>-0.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57477 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       comma_ratio_diff  len_diff  lexical_div_diff  punc_diff  \\\n",
       "0             -0.509879  3.952550         -0.550103   3.333333   \n",
       "1             -0.247947 -0.634638         -0.713681  -0.750000   \n",
       "2             -0.727202 -1.084223          0.816672  -1.250000   \n",
       "3              0.382777  1.921708         -0.593601   1.666667   \n",
       "4              0.069289  0.626335         -0.709755   0.833333   \n",
       "...                 ...       ...               ...        ...   \n",
       "57472          1.690206 -0.190985          1.050461   0.250000   \n",
       "57473          0.436278 -0.007117          0.239260   0.000000   \n",
       "57474         -0.402496  8.451957         -0.398586   4.416667   \n",
       "57475          0.232915 -0.633452          0.009375  -0.333333   \n",
       "57476         -1.322622 -0.608541          3.816926  -0.583333   \n",
       "\n",
       "       repetition_diff  sent_count_diff  contains_company  contains_brace  \\\n",
       "0             0.550103         3.000000               0.0             0.0   \n",
       "1             0.713681        -1.777778               0.0             0.0   \n",
       "2            -0.816672        -0.666667               0.0             0.0   \n",
       "3             0.593601         1.222222               0.0             0.0   \n",
       "4             0.709755         0.222222               0.0             0.0   \n",
       "...                ...              ...               ...             ...   \n",
       "57472        -1.050461         1.000000               0.0             0.0   \n",
       "57473        -0.239260        -1.222222               0.0             0.0   \n",
       "57474         0.398586        10.111111               0.0             0.0   \n",
       "57475        -0.009375        -0.666667               0.0             0.0   \n",
       "57476        -3.816926        -0.444444               0.0             0.0   \n",
       "\n",
       "       contains_knee  contains_progression  contains_apologize  contains_sorry  \n",
       "0                0.0                   0.0                 0.0             0.0  \n",
       "1                0.0                   0.0                 0.0             0.0  \n",
       "2                0.0                   0.0                 0.0             0.0  \n",
       "3                0.0                   0.0                 0.0             0.0  \n",
       "4                0.0                   0.0                 0.0             0.0  \n",
       "...              ...                   ...                 ...             ...  \n",
       "57472            0.0                   0.0                 0.0             0.0  \n",
       "57473            0.0                   0.0                 0.0             0.0  \n",
       "57474            0.0                   0.0                 0.0             0.0  \n",
       "57475            0.0                   0.0                 0.0             0.0  \n",
       "57476            0.0                   0.0                 0.0             0.0  \n",
       "\n",
       "[57477 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2883a9",
   "metadata": {},
   "source": [
    "## SEMANTIC Í≤∞Ìï©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def clean_prompt(s):\n",
    "    return re.sub(r'[\"\\'\\[\\]]', '', str(s))\n",
    "\n",
    "train_df['prompt_clean'] = train_df['prompt'].apply(clean_prompt)\n",
    "test_df['prompt_clean']  = test_df['prompt'].apply(clean_prompt)\n",
    "\n",
    "# Prompt + Response Ìï©ÏÑ±\n",
    "train_df['text_a'] = train_df['prompt_clean'] + ' ' + train_df['response_a']\n",
    "train_df['text_b'] = train_df['prompt_clean'] + ' ' + train_df['response_b']\n",
    "test_df['text_a']  = test_df['prompt_clean'] + ' ' + test_df['response_a']\n",
    "test_df['text_b']  = test_df['prompt_clean'] + ' ' + test_df['response_b']\n",
    "\n",
    "# Embedding Ï∂îÏ∂ú\n",
    "emb_a_train = model.encode(train_df['text_a'].tolist(), show_progress_bar=True)\n",
    "emb_b_train = model.encode(train_df['text_b'].tolist(), show_progress_bar=True)\n",
    "emb_a_test  = model.encode(test_df['text_a'].tolist(), show_progress_bar=True)\n",
    "emb_b_test  = model.encode(test_df['text_b'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# ÏùëÎãµ Í∞Ñ Ï∞®Ïù¥ Î≤°ÌÑ∞ + prompt ÏùòÎØ∏ Ï∂îÍ∞Ä\n",
    "emb_diff_train = emb_a_train - emb_b_train\n",
    "emb_diff_test  = emb_a_test  - emb_b_test\n",
    "\n",
    "prompt_emb_train = model.encode(train_df['prompt_clean'].tolist(), show_progress_bar=True)\n",
    "prompt_emb_test  = model.encode(test_df['prompt_clean'].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hybrid = np.concatenate([emb_diff_train, prompt_emb_train, X_tab.values], axis=1)\n",
    "X_test_hybrid  = np.concatenate([emb_diff_test,  prompt_emb_test,  X_tab.values[:len(test_df)]], axis=1)\n",
    "\n",
    "# Target ÏÉùÏÑ±\n",
    "conditions = [\n",
    "    train_df['winner_model_a'] == 1,\n",
    "    train_df['winner_model_b'] == 1,\n",
    "    train_df['winner_tie'] == 1\n",
    "]\n",
    "y_train = np.select(conditions, [0, 1, 2], default=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9421eb",
   "metadata": {},
   "source": [
    "ÎòêÎäî train_hybrid_data.npz Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80c02604",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train_hybrid, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial', C=0.5)\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "lgb = LGBMClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lexical_models = [(\"lr\", lr), (\"rf\", rf), (\"gb\", gb), (\"lgb\", lgb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_voting = VotingClassifier(\n",
    "    estimators=lexical_models,\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "print(\"üöÄ Training lexical ensemble...\\n\")\n",
    "hybrid_voting.fit(X_train, y_train_split)\n",
    "print(\"‚úÖ Lexical ensemble training complete.\\n\")\n",
    "\n",
    "# Validation check\n",
    "y_val_pred = hybrid_voting.predict(X_val)\n",
    "y_val_prob = hybrid_voting.predict_proba(X_val)\n",
    "\n",
    "acc = accuracy_score(y_val, y_val_pred)\n",
    "loss = log_loss(y_val, y_val_prob)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "print(f\"Validation Log Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb83c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hybrid_voting.predict(X_val)\n",
    "y_prob = hybrid_voting.predict_proba(X_val)\n",
    "\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "loss = log_loss(y_val, y_prob)\n",
    "\n",
    "print(f\"\\nüìä Validation Accuracy: {acc:.4f}\")\n",
    "print(f\"üìâ Validation LogLoss: {loss:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=['A win', 'B win', 'Tie']))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Validation)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
