{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le_SnErWLhPE"
      },
      "source": [
        "# Part 1: DeBERTa + LoRA Fine-Tuning and Saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cPizUI_LhPE"
      },
      "source": [
        "This notebook fine-tunes a DeBERTa-small model using LoRA and saves the resulting adapter model weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0rI8DadLhPF"
      },
      "source": [
        "## 1. Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejZH4gO_N5G-",
        "outputId": "e3e8c2c7-de62-4967-b0db-674bd379080a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n"
          ]
        }
      ],
      "source": [
        "#quantization용 라이브러리인데 quantization 적용을 실패해서 사용하지 않습니다..\n",
        "#!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF5Vj0J_LhPF"
      },
      "source": [
        "## 2. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU3RoGbMLhPF",
        "outputId": "06c3d8e5-422b-4c04-8e0a-d3e225bdf3d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset, ClassLabel, Value\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# 1. [변경] 'datasets'로 CSV 로드 (RAM에 올리지 않음)\n",
        "path = './dataset/'\n",
        "raw_dataset = load_dataset('csv', data_files=path + 'train.csv')\n",
        "\n",
        "# 2. [변경] 전처리 함수 정의 (데이터 생성과 토큰화를 한 번에)\n",
        "model_name = 'microsoft/deberta-v3-small'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # 레이블 생성 (handle batched input)\n",
        "    conditions = [(np.array(examples['winner_model_a']) == 1), (np.array(examples['winner_model_b']) == 1), (np.array(examples['winner_tie']) == 1)]\n",
        "    choices = [0, 1, 2]\n",
        "    examples['labels'] = np.select(conditions, choices, default=-1).tolist()\n",
        "\n",
        "    # 텍스트 생성 (handle batched input)\n",
        "    examples['text'] = [(\"prompt: \" + prompt +\n",
        "                        \"\\n\\nresponse_a: \" + response_a +\n",
        "                        \"\\n\\nresponse_b: \" + response_b) for prompt, response_a, response_b in zip(examples['prompt'], examples['response_a'], examples['response_b'])]\n",
        "\n",
        "    # 토큰화 (여기서 max_length가 중요)\n",
        "    tokenized_inputs = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "    # 토okenization results are already batched, just add the 'labels'\n",
        "    tokenized_inputs['labels'] = examples['labels']\n",
        "\n",
        "    # Return only necessary columns\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "# 3. [변경] .map()으로 전처리 적용 (배치로 처리되어 빠르고 RAM 절약)\n",
        "# batched=True가 핵심입니다.\n",
        "tokenized_dataset = raw_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# 4. [변경] 불필요한 원본 컬럼 제거 (필수!)\n",
        "# 'label' -> 'labels'로 이름이 바뀌었으므로 원본 'label'도 제거\n",
        "tokenized_dataset = tokenized_dataset['train'].remove_columns([\n",
        "    'prompt', 'response_a', 'response_b', 'text',\n",
        "    'winner_model_a', 'winner_model_b', 'winner_tie' # 'label' is not in the original csv columns\n",
        "])\n",
        "\n",
        "# 5. [변경] 유효하지 않은 레이블(-1) 필터링\n",
        "tokenized_dataset = tokenized_dataset.filter(lambda example: example['labels'] != -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Shoa9TlXUXHb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, ClassLabel, Value\n",
        "\n",
        "# Cast 'labels' column to ClassLabel for stratification\n",
        "tokenized_dataset = tokenized_dataset.cast_column(\"labels\", ClassLabel(num_classes=3))\n",
        "\n",
        "# 6. [변경] 데이터셋 분할\n",
        "final_datasets = tokenized_dataset.train_test_split(test_size=0.2, stratify_by_column=\"labels\")\n",
        "\n",
        "# (참고) 이제 더 이상 수동으로 PreferenceDataset을 만들 필요가 없습니다!\n",
        "train_dataset = final_datasets[\"train\"]\n",
        "val_dataset = final_datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h1blfWuLhPG"
      },
      "source": [
        "## 4. LoRA Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "e--vmZMSLhPG",
        "outputId": "09df06ed-7500-42ce-929b-b44951b73536"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2001' max='2874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2001/2874 20:46 < 09:04, 1.60 it/s, Epoch 0.70/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.102800</td>\n",
              "      <td>1.097234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.099100</td>\n",
              "      <td>1.091948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.091200</td>\n",
              "      <td>1.087873</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1154' max='1437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1154/1437 01:47 < 00:26, 10.68 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2874' max='2874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2874/2874 31:24, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.102800</td>\n",
              "      <td>1.097234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.099100</td>\n",
              "      <td>1.091948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.091200</td>\n",
              "      <td>1.087873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.090600</td>\n",
              "      <td>1.087077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.091700</td>\n",
              "      <td>1.085878</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2874, training_loss=1.0943977711676887, metrics={'train_runtime': 1885.2869, 'train_samples_per_second': 24.389, 'train_steps_per_second': 1.524, 'total_flos': 6102049267132416.0, 'train_loss': 1.0943977711676887, 'epoch': 1.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig\n",
        "import torch\n",
        "\n",
        "model_name = 'microsoft/deberta-v3-small'\n",
        "\n",
        "# Remove 8-bit quantization configuration for testing\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_8bit=True,\n",
        "#     bnb_8bit_quant_type=\"nf4\",  # You can experiment with \"nf4\" or \"fp4\"\n",
        "#     bnb_8bit_compute_dtype=torch.bfloat16, # Or torch.float16, depending on your GPU\n",
        "#     bnb_8bit_use_double_quant=True, # Optional: use double quantization\n",
        "# )\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    # Remove quantization_config argument\n",
        "    # quantization_config=bnb_config, # Use the quantization_config argument\n",
        "    device_map='auto'\n",
        "    )\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=4, lora_alpha=32, target_modules=['query_proj', 'value_proj'],\n",
        "    lora_dropout=0.05, bias='none', task_type=\"SEQ_CLS\"\n",
        "    )\n",
        "\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_lora', num_train_epochs=1, per_device_train_batch_size=16, # Reduced batch size\n",
        "    fp16=True,gradient_accumulation_steps=1, # Add gradient accumulation\n",
        "    per_device_eval_batch_size=8, warmup_steps=500, weight_decay=0.01,\n",
        "    logging_dir='./logs_lora', eval_strategy=\"steps\", eval_steps=500, # Changed evaluation_strategy to eval_strategy\n",
        "    save_steps=500, load_best_model_at_end=True,report_to=\"none\"\n",
        "    )\n",
        "\n",
        "trainer = Trainer(model=lora_model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS_83oGlLhPH"
      },
      "source": [
        "## 5. Save LoRA Model Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKV6HoxYLhPH",
        "outputId": "83f3d198-5a5e-40e8-ffc5-640c1ee796fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA model adapters saved to deberta_lora_model\n"
          ]
        }
      ],
      "source": [
        "model_save_path = 'deberta_lora_model'\n",
        "lora_model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path) # Save tokenizer with the adapters\n",
        "\n",
        "print(f\"LoRA model adapters saved to {model_save_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "vscode": {
      "interpreter": {
        "hash": "39d32efba01c30b0368d314f541dd447b1e0abc1c94139e3532a931796c9cc34"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
