{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":625240,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":470561,"modelId":486456},{"sourceId":343362,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":287121,"modelId":307935}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Part 2: Submission with Fine-Tuned LoRA Model","metadata":{}},{"cell_type":"markdown","source":"This notebook loads the base DeBERTa model and the fine-tuned LoRA adapters, calibrates the probabilities, and generates the final submission.","metadata":{}},{"cell_type":"markdown","source":"## 1. Install Libraries","metadata":{}},{"cell_type":"code","source":"#not needed for kaggle\n#!pip install -q transformers peft accelerate bitsandbytes torch scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:58:47.438890Z","iopub.execute_input":"2025-10-31T10:58:47.439228Z","iopub.status.idle":"2025-10-31T10:58:47.442900Z","shell.execute_reply.started":"2025-10-31T10:58:47.439207Z","shell.execute_reply":"2025-10-31T10:58:47.442059Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## 2. Load Data and Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom peft import PeftModel\n\n# Load data for validation set (for calibration) and test set\n#path = '../dataset/'\npath = '/kaggle/input/llm-classification-finetuning/'\ntrain_df = pd.read_csv(path + 'train.csv')\ntest_df = pd.read_csv(path + 'test.csv')\nsubmission_df = pd.read_csv(path + 'sample_submission.csv')\n\n# --- Recreate the validation set to match the one used in training ---\nconditions = [train_df['winner_model_a'] == 1, train_df['winner_model_b'] == 1, train_df['winner_tie'] == 1]\nchoices = [0, 1, 2]\ntrain_df['label'] = np.select(conditions, choices, default=-1)\ntrain_df = train_df[train_df['label'] != -1].copy()\n\ndef create_text(row):\n    return f\"\"\"prompt: {row['prompt']}\n\nresponse_a: {row['response_a']}\n\nresponse_b: {row['response_b']}\"\"\"\n\ntrain_df['text'] = train_df.apply(create_text, axis=1)\ntest_df['text'] = test_df.apply(create_text, axis=1)\n\n_, val_texts, _, val_labels = train_test_split(\n    train_df['text'], train_df['label'], test_size=0.2, random_state=42, stratify=train_df['label']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:58:47.444162Z","iopub.execute_input":"2025-10-31T10:58:47.444338Z","iopub.status.idle":"2025-10-31T10:58:49.712484Z","shell.execute_reply.started":"2025-10-31T10:58:47.444324Z","shell.execute_reply":"2025-10-31T10:58:49.711862Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# --- Load Model and Adapters ---\nmodel_name = '/kaggle/input/deberta-v3-small/transformers/default/1'\nlora_model_path = '/kaggle/input/deberta-lora-model/transformers/default/1/deberta_lora_model'\n\n# Load the base model\nbase_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3, device_map='auto')\n\n# Load the LoRA adapters and apply them to the base model\nlora_model = PeftModel.from_pretrained(base_model, lora_model_path)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(lora_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:59:24.834881Z","iopub.execute_input":"2025-10-31T10:59:24.835580Z","iopub.status.idle":"2025-10-31T10:59:25.572688Z","shell.execute_reply.started":"2025-10-31T10:59:24.835556Z","shell.execute_reply":"2025-10-31T10:59:25.572057Z"}},"outputs":[{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-small/transformers/default/1 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['target_parameters'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 3. Probability Calibration","metadata":{}},{"cell_type":"code","source":"from sklearn.isotonic import IsotonicRegression\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Create a dataset for the validation text\nval_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\nclass TempDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings): self.encodings = encodings\n    def __getitem__(self, idx): return {key: val[idx] for key, val in self.encodings.items()}\n    def __len__(self): return len(self.encodings['input_ids'])\nval_temp_dataset = TempDataset(val_encodings)\n\n# We need a dummy trainer to run predictions easily\ndummy_trainer = Trainer(model=lora_model)\nval_predictions = dummy_trainer.predict(val_temp_dataset)\nval_probs = torch.nn.functional.softmax(torch.from_numpy(val_predictions.predictions), dim=-1).numpy()\n\n# Train calibrators\ncalibrators = {}\nfor i in range(3):\n    iso_reg = IsotonicRegression(out_of_bounds='clip')\n    y_cal = (val_labels.to_numpy() == i).astype(int)\n    iso_reg.fit(val_probs[:, i], y_cal)\n    calibrators[i] = iso_reg\n\nprint(\"Calibration models trained.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T11:00:08.925733Z","iopub.execute_input":"2025-10-31T11:00:08.926414Z","iopub.status.idle":"2025-10-31T11:02:57.137367Z","shell.execute_reply.started":"2025-10-31T11:00:08.926382Z","shell.execute_reply":"2025-10-31T11:02:57.136586Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Calibration models trained.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 4. Generate Final Submission","metadata":{}},{"cell_type":"code","source":"# Create dataset for the test text\ntest_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\ntest_temp_dataset = TempDataset(test_encodings)\n\n# Predict on the test set\ntest_predictions = dummy_trainer.predict(test_temp_dataset)\ntest_probs = torch.nn.functional.softmax(torch.from_numpy(test_predictions.predictions), dim=-1).numpy()\n\n# Apply calibration\ncalibrated_probs = np.zeros_like(test_probs)\nfor i in range(3):\n    calibrated_probs[:, i] = calibrators[i].predict(test_probs[:, i])\n\n# Normalize probabilities\ncalibrated_probs_sum = np.sum(calibrated_probs, axis=1, keepdims=True)\nnormalized_probs = calibrated_probs / (calibrated_probs_sum + 1e-9)\n\n# Create submission file\nsubmission_df['winner_model_a'] = normalized_probs[:, 0]\nsubmission_df['winner_model_b'] = normalized_probs[:, 1]\nsubmission_df['winner_tie'] = normalized_probs[:, 2]\n\nsubmission_df.to_csv('submission.csv', index=False)\n\nsubmission_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T11:04:14.978977Z","iopub.execute_input":"2025-10-31T11:04:14.979683Z","iopub.status.idle":"2025-10-31T11:04:15.064346Z","shell.execute_reply.started":"2025-10-31T11:04:14.979642Z","shell.execute_reply":"2025-10-31T11:04:15.063388Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"        id  winner_model_a  winner_model_b  winner_tie\n0   136060        0.212494        0.349897    0.437609\n1   211333        0.384959        0.353800    0.261241\n2  1233961        0.359485        0.342486    0.298029","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>0.212494</td>\n      <td>0.349897</td>\n      <td>0.437609</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>0.384959</td>\n      <td>0.353800</td>\n      <td>0.261241</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>0.359485</td>\n      <td>0.342486</td>\n      <td>0.298029</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18}]}