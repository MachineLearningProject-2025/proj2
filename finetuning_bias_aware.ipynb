{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with Bias-Aware Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook integrates bias-aware techniques into the fine-tuning process. It includes:\n",
    "1. Analysis of verbosity and position bias.\n",
    "2. Data augmentation to mitigate position bias.\n",
    "3. Fine-tuning a DeBERTa-small model using LoRA on the augmented dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft accelerate bitsandbytes torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Analyze Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "submission_df = pd.read_csv('dataset/sample_submission.csv')\n",
    "\n",
    "# --- Verbosity Bias Analysis ---\n",
    "train_df['len_a'] = train_df['response_a'].str.len()\n",
    "train_df['len_b'] = train_df['response_b'].str.len()\n",
    "train_df['word_count_a'] = train_df['response_a'].apply(lambda x: len(str(x).split()))\n",
    "train_df['word_count_b'] = train_df['response_b'].apply(lambda x: len(str(x).split()))\n",
    "print('--- Verbosity Analysis ---')\n",
    "print(f\"Avg word count for A: {train_df['word_count_a'].mean():.2f}\")\n",
    "print(f\"Avg word count for B: {train_df['word_count_b'].mean():.2f}\")\n",
    "\n",
    "# --- Position Bias Analysis ---\n",
    "model_a_wins = train_df['winner_model_a'].sum()\n",
    "model_b_wins = train_df['winner_model_b'].sum()\n",
    "ties = train_df['winner_tie'].sum()\n",
    "total = len(train_df)\n",
    "print('--- Position Bias Analysis ---')\n",
    "print(f'Total samples: {total}')\n",
    "print(f'Model A wins: {model_a_wins} ({model_a_wins/total:.2%})')\n",
    "print(f'Model B wins: {model_b_wins} ({model_b_wins/total:.2%})')\n",
    "print(f'Ties: {ties} ({ties/total:.2%})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare original labels\n",
    "conditions = [train_df['winner_model_a'] == 1, train_df['winner_model_b'] == 1, train_df['winner_tie'] == 1]\n",
    "choices = [0, 1, 2] # 0: model_a, 1: model_b, 2: tie\n",
    "train_df['label'] = np.select(conditions, choices, default=-1)\n",
    "train_df = train_df[train_df['label'] != -1].copy()\n",
    "\n",
    "# --- Augmentation to mitigate position bias ---\n",
    "augmented_rows = []\n",
    "for _, row in train_df.iterrows():\n",
    "    # Keep original row\n",
    "    augmented_rows.append(row.to_dict())\n",
    "    \n",
    "    # Create swapped row if not a tie\n",
    "    if row['label'] != 2:\n",
    "        new_row = row.to_dict()\n",
    "        new_row['response_a'], new_row['response_b'] = row['response_b'], row['response_a']\n",
    "        new_row['label'] = 1 - row['label'] # Flip label (0 becomes 1, 1 becomes 0)\n",
    "        augmented_rows.append(new_row)\n",
    "\n",
    "augmented_train_df = pd.DataFrame(augmented_rows).reset_index(drop=True)\n",
    "print(f'Original training size: {len(train_df)}')\n",
    "print(f'Augmented training size: {len(augmented_train_df)}')\n",
    "\n",
    "# Create combined text field\n",
    "def create_text(row):\n",
    "    return f\"\"\"prompt: {row['prompt']}\n",
    "\n",
    "response_a: {row['response_a']}\n",
    "\n",
    "response_b: {row['response_b']}\"\"\"\n",
    "\n",
    "augmented_train_df['text'] = augmented_train_df.apply(create_text, axis=1)\n",
    "test_df['text'] = test_df.apply(create_text, axis=1)\n",
    "\n",
    "# Split augmented train data for validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    augmented_train_df['text'], augmented_train_df['label'], test_size=0.1, random_state=42, stratify=augmented_train_df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'microsoft/deberta-v3-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "class PreferenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = PreferenceDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = PreferenceDataset(val_encodings, val_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Setup and LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    load_in_8bit=True,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=['query_proj', 'value_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_bias_aware',\n",
    "    num_train_epochs=1, # A single epoch for a quick baseline\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_bias_aware',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "test_dataset = TestDataset(test_encodings)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "probs = torch.nn.functional.softmax(torch.from_numpy(predictions.predictions), dim=-1).numpy()\n",
    "\n",
    "submission_df['winner_model_a'] = probs[:, 0]\n",
    "submission_df['winner_model_b'] = probs[:, 1]\n",
    "submission_df['winner_tie'] = probs[:, 2]\n",
    "\n",
    "submission_df.to_csv('submission_finetuned_bias_aware.csv', index=False)\n",
    "\n",
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
