# [MLP Project 2] Predicting Human Preferences for LLM Response Enhancement

## üìå 1. Project Overview

| Detail                | Description                                                                                                                                                                                                           |
| :-------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Course**            | CS 53744 Machine Learning Project                                                                                                                                                                                     |
| **Task**              | Multiclass classification to predict **human preference** between two LLM responses (*A win*, *B win*, *Tie*).                                                                                                        |
| **Dataset**           | [Kaggle Competition ‚Äì LLM Classification Finetuning](https://www.kaggle.com/competitions/llm-classification-finetuning)                                                                                               |
| **Goal**              | Build models that approximate **human judgment** in pairwise response evaluation through lexical, semantic, and contextual modeling.                                                                                  |
| **Evaluation Metric** | Log Loss (Kaggle official), Accuracy, Macro F1                                                                                                                                                                        |
| **Final Model**       | **DeBERTa-v3-small + LoRA Fine-Tuning + Isotonic Calibration**                                                                                                                                                        |
| **Baseline Models**   | Logistic Regression (Lexical only), SentenceTransformer all-MiniLM-L6-v2 (Semantic only), Hybrid Stacking (Lexical + Semantic)                                                                                        |
| **Key Insight**       | While hybrid ensembling achieved higher raw accuracy, the **DeBERTa + LoRA model** provided more balanced probability calibration and superior performance in **Tie prediction**, reflecting human-aligned reasoning. |


-----

## üë• 2. Team Information

| Role | Name | GitHub ID |
| :--- | :--- | :--- |
| Member | Î∞ïÏõêÍ∑ú | `@keiro23` |
| Member | Ïù¥Ïú†Ï†ï | `@yousrchive` |
| Member | Ï†ïÏäπÌôò | `@whan0767` |

-----

## üèÜ 3. Final Performance Summary

The final submitted model is based on **DeBERTa-v3-small** fine-tuned with **LoRA (Low-Rank Adaptation)** and post-processed through **Isotonic Regression calibration**.
This setup enables **token-level contextual comparison** between paired responses while preserving computational efficiency in the no-internet Kaggle GPU environment.

| Metric                     | Value                                              | Note                                           |
| :------------------------- | :------------------------------------------------- | :--------------------------------------------- |
| **Kaggle Public Log Loss** | **1.09**                                           | Final submission score on Kaggle leaderboard   |
| **Validation Accuracy**    | **0.42**                                           | Evaluated on 20% stratified validation split   |
| **Validation Macro F1**    | **0.42**                                           | Balanced across A-win, B-win, and Tie          |
| **Calibration Gain**       | **‚âà +10% relative improvement in accuracy**        | Achieved through post-hoc isotonic calibration |
| **Final Model**            | **DeBERTa-v3-small + LoRA + Isotonic Calibration** | Fine-tuned checkpoint `checkpoint-22992`       |

**Key Strengths:**

* **Improved Tie recognition** through probabilistic calibration and contextual understanding
* **Lightweight training** using LoRA ‚Äî only low-rank attention weights updated
* **Fully reproducible** in Kaggle‚Äôs no-internet environment (pre-mounted datasets and model weights)
* **Human-aligned reasoning:** prioritizes balanced, context-aware prediction rather than lexical bias

**Notes:**
Although the Hybrid Stacking model achieved slightly higher raw validation accuracy (0.48),
the DeBERTa + LoRA model exhibited superior alignment with human judgment,
especially in **contextually equivalent (Tie)** cases.
This makes it a more robust and scalable final model for preference modeling tasks.


-----

## ‚öôÔ∏è 4. How to Reproduce Results

This guide provides the steps to reproduce the results for the two final models.

### 4.1. Environment Setup & Dependencies

1.  **Create and Activate Virtual Environment:**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # Linux/macOS
    ```
2.  **Install Required Packages:** Install all dependencies from the centralized `requirements.txt` file in the project root.
    ```bash
    pip install -r requirements.txt
    ```
3.  **Download NLTK Data:** Some lexical features depend on NLTK. Run this command in a Python interpreter to download the necessary data:
    ```python
    import nltk
    nltk.download('punkt')
    nltk.download('averaged_perceptron_tagger')
    ```

### 4.2. Data Preparation

1.  **Download Files:** Obtain the three CSV files from the Kaggle competition page.
2.  **Place Data:** Save the downloaded files (`train.csv`, `test.csv`, `sample_submission.csv`) into the **`dataset/`** folder within the `PROJ2` root directory.

### 4.3. Model 1: Hybrid Stacking Classifier

This model combines lexical features and sentence embeddings. The entire workflow is managed by `main.py`.

1.  **Change Directory:** Navigate to the script's location.
    ```bash
    cd model/model_hybrid_stacking
    ```
2.  **Run Main Script:** Execute the full pipeline. This will generate a submission file in the same directory.
    ```bash
    python main.py
    ```
3.  **Output:** The script saves predictions to `submission.csv`.

### 4.4. Model 2: DeBERTa with LoRA Fine-Tuning

This model fine-tunes a `DeBERTa-v3-small` model using LoRA. The process involves two stages: training and prediction.

1.  **Change Directory:** Navigate to the scripts' location.
    ```bash
    cd model/model_derberta_lora
    ```
2.  **Stage 1: Train the Model:**
    Execute the training script. This will fine-tune the model and save the LoRA adapter in the `results_lora_strategic/` directory.
    ```bash
    python train.py
    ```
3.  **Stage 2: Generate Predictions:**
    After training is complete, run the prediction script. This uses the trained LoRA weights and generates a submission file.
    ```bash
    python predict.py
    ```
4.  **Output:** The script saves predictions to `submission.csv`.

-----

## üìÅ 5. Project Directory Structure

```
PROJ2/
‚îú‚îÄ‚îÄ dataset/                     # Input Data Location
‚îÇ   ‚îú‚îÄ‚îÄ train.csv
‚îÇ   ‚îî‚îÄ‚îÄ test.csv
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ model_derberta_lora/     # DeBERTa LoRA Model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ model_hybrid_stacking/   # Hybrid Stacking Model
‚îÇ       ‚îú‚îÄ‚îÄ main.py
‚îÇ       ‚îú‚îÄ‚îÄ model_trainer.py
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ experiments/                 # Jupyter Notebooks for Intermediate Steps
‚îú‚îÄ‚îÄ .venv/                       # Python Virtual Environment
‚îî‚îÄ‚îÄ requirements.txt             # All Python Dependencies
```